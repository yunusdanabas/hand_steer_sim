<?xml version="1.0"?>
<launch>

  <!-- sign_control -->
  <!-- Main launcher: ties together camera, simulation, gesture recognition, and control -->

  <!-- Topics for gesture output and robot control -->
  <arg name="publish_gesture_topic" default="/gesture/hand_sign"/>
  <arg name="control_topic"        default="/robot_diff_drive_controller/cmd_vel"/>

  <!-- Camera-selection flags & defaults -->
  <arg name="mode"         default="webcam"/>   <!-- Camera Mode -->
  <arg name="camera_name"       default="/dev/video0"/>  <!-- Default USB/integrated cam -->
  <arg name="realsense_device"  default="/dev/video6"/>  <!-- D435 color UVC device -->

  <!-- Drive the simulated car -->
  <arg name="drive_car" default="false"/>

  <!-- Camera interface (delegates to camera_interface.launch) -->
  <include file="$(find hand_steer_sim)/launch/camera_interface.launch">
    <arg name="mode"      value="$(arg mode)"/>
    <arg name="camera_name"        value="$(arg camera_name)"/>
    <arg name="realsense_device" value="$(arg realsense_device)"/>
  </include>

  <!-- Launch Gazebo robot if requested -->
  <group if="$(arg drive_car)">
    <include file="$(find hand_steer_sim)/launch/drive_car.launch"/>
  </group>

  <!-- Gesture recognition pipeline -->
  <include file="$(find hand_steer_sim)/launch/gesture_recognition.launch"/>

  <!-- Map recognized gestures to velocity commands -->
  <node name="gesture_to_twist_node"
        pkg="hand_steer_sim"
        type="gesture_to_twist_node.py"
        output="screen">
    <param name="publish_gesture_topic" value="$(arg publish_gesture_topic)"/>
    <param name="control_topic"         value="$(arg control_topic)"/>
  </node>

</launch>
